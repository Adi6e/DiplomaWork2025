{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate jiwer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:13:22.114637Z","iopub.execute_input":"2025-05-23T11:13:22.115325Z","iopub.status.idle":"2025-05-23T11:13:28.421905Z","shell.execute_reply.started":"2025-05-23T11:13:22.115299Z","shell.execute_reply":"2025-05-23T11:13:28.421152Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nCollecting jiwer\n  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\nCollecting rapidfuzz>=3.9.7 (from jiwer)\n  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nCollecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jiwer-3.1.0-py3-none-any.whl (22 kB)\nDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, fsspec, jiwer, evaluate\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.3 fsspec-2025.3.0 jiwer-3.1.0 rapidfuzz-3.13.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#!/usr/bin/env python3\n\"\"\"\nСкрипт для файнтюнинга модели Whisper на русскоязычном датасете Golos\nс расчетом метрик до и после обучения.\n\"\"\"\n\nimport os\nimport torch\nimport torchaudio\nimport numpy as np\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\nimport json\nimport logging\nfrom pathlib import Path\nimport evaluate\nfrom datasets import load_dataset, DatasetDict, Audio\nimport jiwer\nfrom collections import Counter\nimport re\nfrom transformers import (\n    WhisperFeatureExtractor,\n    WhisperTokenizer,\n    WhisperProcessor,\n    WhisperForConditionalGeneration,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    TrainerCallback,\n)\nfrom transformers.models.whisper.english_normalizer import BasicTextNormalizer\n\n# Настройка логирования\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Проверка доступности GPU\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nlogger.info(f\"Используется устройство: {device}\")\n\n@dataclass\nclass ModelArguments:\n    model_name_or_path: str = \"openai/whisper-small\"\n    cache_dir: str = \"./cache\"\n    use_fast_tokenizer: bool = True\n    model_revision: str = \"main\"\n    use_auth_token: bool = False\n\n@dataclass\nclass DataArguments:\n    dataset_name: str = \"bond005/sberdevices_golos_100h_farfield\"\n    dataset_config_name: str = None\n    train_split_name: str = \"train\"\n    eval_split_name: str = \"validation\"\n    audio_column_name: str = \"audio\"\n    text_column_name: str = \"sentence\"  # Обновлено для нового датасета\n    max_train_samples: int = None\n    max_eval_samples: int = None\n    max_duration_in_seconds: float = 30.0\n\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    \"\"\"Коллатор данных для обучения Whisper\"\"\"\n    \n    def __init__(self, processor, decoder_start_token_id):\n        self.processor = processor\n        self.decoder_start_token_id = decoder_start_token_id\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # Разделяем входные данные на аудио и текст\n        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n\n        # Создаем батч для входных аудио данных\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n\n        # Создаем батч для лейблов\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n\n        # Заменяем паддинг токены на -100 для игнорирования в loss\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n\n        # Если bos токен добавлен в начале, удаляем его так как он будет добавлен позже\n        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n\n        batch[\"labels\"] = labels\n\n        return batch\n\ndef load_golos_dataset():\n    \"\"\"Загрузка датасета Golos от bond005\"\"\"\n    try:\n        # Загружаем датасет bond005/sberdevices_golos_100h_farfield\n        logger.info(\"Загружаем датасет bond005/sberdevices_golos_100h_farfield...\")\n        \n        # Загружаем train и validation splits\n        dataset_dict = {}\n        \n        # Пробуем загрузить training split\n        try:\n            train_dataset = load_dataset(\n                \"bond005/sberdevices_golos_100h_farfield\",\n                split=\"train\",\n                streaming=False\n            )\n            logger.info(f\"Training split загружен: {len(train_dataset)} примеров\")\n            dataset_dict[\"train\"] = train_dataset\n        except Exception as e:\n            logger.warning(f\"Не удалось загрузить training split: {e}\")\n        \n        # Пробуем загрузить validation split\n        try:\n            val_dataset = load_dataset(\n                \"bond005/sberdevices_golos_100h_farfield\",\n                split=\"validation\",\n                streaming=False\n            )\n            logger.info(f\"Validation split загружен: {len(val_dataset)} примеров\")\n            dataset_dict[\"validation\"] = val_dataset\n        except Exception as e:\n            logger.warning(f\"Не удалось загрузить validation split: {e}\")\n            \n            # Если нет validation split, создаем его из train\n            if \"train\" in dataset_dict:\n                logger.info(\"Создаем validation split из training данных...\")\n                train_val_split = dataset_dict[\"train\"].train_test_split(test_size=0.1, seed=42)\n                dataset_dict[\"train\"] = train_val_split[\"train\"]\n                dataset_dict[\"validation\"] = train_val_split[\"test\"]\n        \n        # Если нет ни одного split, пробуем загрузить без указания split\n        if not dataset_dict:\n            logger.info(\"Пробуем загрузить датасет без указания split...\")\n            full_dataset = load_dataset(\"bond005/sberdevices_golos_100h_farfield\")\n            \n            # Проверяем доступные splits\n            logger.info(f\"Доступные splits: {list(full_dataset.keys())}\")\n            \n            # Используем первый доступный split как train\n            if full_dataset:\n                first_split = list(full_dataset.keys())[0]\n                logger.info(f\"Используем split '{first_split}' как основной\")\n                \n                # Создаем train/validation split\n                train_val_split = full_dataset[first_split].train_test_split(test_size=0.1, seed=42)\n                dataset_dict[\"train\"] = train_val_split[\"train\"]\n                dataset_dict[\"validation\"] = train_val_split[\"test\"]\n        \n        if not dataset_dict:\n            raise ValueError(\"Не удалось загрузить ни один split датасета\")\n        \n        # Проверяем структуру данных\n        sample = dataset_dict[\"train\"][0]\n        logger.info(f\"Пример структуры данных: {list(sample.keys())}\")\n        \n        # Проверяем наличие нужных колонок\n        required_columns = [\"audio\", \"sentence\"]\n        available_columns = list(sample.keys())\n        \n        for col in required_columns:\n            if col not in available_columns:\n                logger.warning(f\"Колонка '{col}' не найдена. Доступные колонки: {available_columns}\")\n                \n                # Пробуем найти альтернативные названия\n                if col == \"sentence\":\n                    alternatives = [\"text\", \"transcription\", \"transcript\", \"target_text\"]\n                    for alt in alternatives:\n                        if alt in available_columns:\n                            logger.info(f\"Используем колонку '{alt}' вместо '{col}'\")\n                            break\n                    else:\n                        logger.error(f\"Не найдена подходящая текстовая колонка\")\n        \n        logger.info(\"Датасет Golos загружен успешно\")\n        return DatasetDict(dataset_dict)\n        \n    except Exception as e:\n        logger.error(f\"Ошибка загрузки датасета Golos: {e}\")\n        logger.info(\"Создается демонстрационный датасет...\")\n        return create_dummy_dataset()\n\ndef create_dummy_dataset():\n    \"\"\"Создание реалистичного демонстрационного датасета для тестирования\"\"\"\n    logger.warning(\"Создается демонстрационный датасет с синтезированной русской речью\")\n    \n    from datasets import Dataset\n    \n    # Создаем более реалистичные аудио данные\n    sample_rate = 16000\n    \n    # Русские фразы для синтеза\n    russian_phrases = [\n        \"Привет как дела\",\n        \"Сегодня хорошая погода\",\n        \"Я изучаю машинное обучение\",\n        \"Whisper работает с русским языком\",\n        \"Нейронные сети очень интересны\",\n        \"Москва столица России\",\n        \"Искусственный интеллект развивается быстро\",\n        \"Давайте попробуем распознать речь\",\n        \"Это тестовый пример для обучения\",\n        \"Русский язык имеет сложную грамматику\",\n        \"Автоматическое распознавание речи\",\n        \"Файнтюнинг модели на русских данных\",\n        \"Качество распознавания улучшается\",\n        \"Глубокое обучение показывает хорошие результаты\",\n        \"Обработка естественного языка\"\n    ]\n    \n    dummy_data = []\n    for i in range(200):  # Больше примеров для лучшего обучения\n        phrase_idx = i % len(russian_phrases)\n        phrase = russian_phrases[phrase_idx]\n        \n        # Создаем более реалистичный аудио сигнал\n        # Имитируем речевой сигнал с основной частотой и формантами\n        duration = len(phrase.split()) * 0.4 + np.random.uniform(0.5, 1.0)  # Реалистичная длительность\n        num_samples = int(duration * sample_rate)\n        \n        # Создаем базовый сигнал с речевыми характеристиками\n        t = np.linspace(0, duration, num_samples)\n        fundamental_freq = np.random.uniform(80, 200)  # Основная частота голоса\n        \n        # Имитируем речевой сигнал\n        signal = np.zeros(num_samples)\n        for harmonic in range(1, 6):\n            amplitude = 1.0 / harmonic\n            signal += amplitude * np.sin(2 * np.pi * fundamental_freq * harmonic * t)\n        \n        # Добавляем формантные частоты\n        for formant_freq in [500, 1500, 2500]:\n            formant_signal = 0.3 * np.sin(2 * np.pi * formant_freq * t)\n            signal += formant_signal\n        \n        # Добавляем огибающую и шум\n        envelope = np.exp(-t * 0.5)  # Экспоненциальная огибающая\n        noise = 0.1 * np.random.randn(num_samples)\n        signal = signal * envelope + noise\n        \n        # Нормализация\n        signal = signal / np.max(np.abs(signal)) * 0.7\n        audio_array = signal.astype(np.float32)\n        \n        dummy_data.append({\n            \"audio\": {\"array\": audio_array, \"sampling_rate\": sample_rate},\n            \"sentence\": phrase  # Изменено с \"transcription\" на \"sentence\"\n        })\n    \n    dataset = Dataset.from_list(dummy_data)\n    \n    # Создаем train/validation split\n    dataset = dataset.train_test_split(test_size=0.15, seed=42)\n    \n    return DatasetDict({\n        \"train\": dataset[\"train\"],\n        \"validation\": dataset[\"test\"]\n    })\n\ndef prepare_dataset(batch, processor, normalizer, text_column_name=\"sentence\"):\n    \"\"\"Предобработка данных\"\"\"\n    # Загружаем аудио\n    audio = batch[\"audio\"]\n    \n    # Вычисляем log-Mel спектрограммы\n    input_features = processor.feature_extractor(\n        audio[\"array\"], \n        sampling_rate=audio[\"sampling_rate\"]\n    ).input_features[0]\n    \n    # Нормализация и токенизация текста\n    transcription = batch[text_column_name]\n    if normalizer:\n        transcription = normalizer(transcription)\n    \n    # Кодируем текст\n    labels = processor.tokenizer(transcription).input_ids\n    \n    return {\n        \"input_features\": input_features,\n        \"labels\": labels\n    }\n\ndef compute_metrics(eval_preds, processor, normalizer, metrics_dict):\n    \"\"\"Вычисление расширенного набора метрик для ASR\"\"\"\n    pred_ids, label_ids = eval_preds\n    \n    # Заменяем -100 на pad_token_id\n    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n    \n    # Декодируем предсказания и истинные лейблы\n    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n    \n    # Нормализация для более справедливого сравнения\n    if normalizer:\n        pred_str = [normalizer(pred) for pred in pred_str]\n        label_str = [normalizer(label) for label in label_str]\n    \n    # Основные метрики\n    results = {}\n    \n    # 1. WER (Word Error Rate)\n    wer = 100 * metrics_dict[\"wer\"].compute(predictions=pred_str, references=label_str)\n    results[\"wer\"] = wer\n    \n    # 2. CER (Character Error Rate)\n    cer = 100 * metrics_dict[\"cer\"].compute(predictions=pred_str, references=label_str)\n    results[\"cer\"] = cer\n    \n    # 3. BLEU Score (для оценки качества текста)\n    try:\n        bleu = metrics_dict[\"bleu\"].compute(predictions=pred_str, references=[[ref] for ref in label_str])\n        results[\"bleu\"] = bleu[\"bleu\"] * 100\n    except:\n        results[\"bleu\"] = 0.0\n    \n    # 4. Детальные метрики с использованием jiwer\n    try:\n        # Объединяем все предсказания и референсы для общей статистики\n        all_preds = \" \".join(pred_str)\n        all_refs = \" \".join(label_str)\n        \n        # Подсчет операций редактирования\n        measures = jiwer.compute_measures(all_refs, all_preds)\n        \n        results[\"substitutions\"] = measures[\"substitutions\"]\n        results[\"deletions\"] = measures[\"deletions\"] \n        results[\"insertions\"] = measures[\"insertions\"]\n        results[\"hits\"] = measures[\"hits\"]\n        \n        # Дополнительные метрики\n        total_words = measures[\"substitutions\"] + measures[\"deletions\"] + measures[\"hits\"]\n        if total_words > 0:\n            results[\"substitution_rate\"] = (measures[\"substitutions\"] / total_words) * 100\n            results[\"deletion_rate\"] = (measures[\"deletions\"] / total_words) * 100\n            results[\"insertion_rate\"] = (measures[\"insertions\"] / (total_words + measures[\"insertions\"])) * 100\n        else:\n            results[\"substitution_rate\"] = 0.0\n            results[\"deletion_rate\"] = 0.0\n            results[\"insertion_rate\"] = 0.0\n            \n    except Exception as e:\n        logger.warning(f\"Ошибка при вычислении детальных метрик: {e}\")\n        results.update({\n            \"substitutions\": 0, \"deletions\": 0, \"insertions\": 0, \"hits\": 0,\n            \"substitution_rate\": 0.0, \"deletion_rate\": 0.0, \"insertion_rate\": 0.0\n        })\n    \n    # 5. Длина предсказаний (для анализа)\n    avg_pred_length = sum(len(pred.split()) for pred in pred_str) / len(pred_str)\n    avg_ref_length = sum(len(ref.split()) for ref in label_str) / len(label_str)\n    results[\"avg_prediction_length\"] = avg_pred_length\n    results[\"avg_reference_length\"] = avg_ref_length\n    results[\"length_ratio\"] = avg_pred_length / avg_ref_length if avg_ref_length > 0 else 0.0\n    \n    # 6. Точность на уровне предложений (Sentence Accuracy)\n    exact_matches = sum(1 for pred, ref in zip(pred_str, label_str) if pred.strip() == ref.strip())\n    sentence_accuracy = (exact_matches / len(pred_str)) * 100\n    results[\"sentence_accuracy\"] = sentence_accuracy\n    \n    return results\n\ndef evaluate_model(model, processor, eval_dataset, normalizer, metrics_dict, max_samples=50):\n    \"\"\"Оценка модели на валидационном наборе с расширенными метриками\"\"\"\n    model.eval()\n    \n    predictions = []\n    references = []\n    \n    # Ограничиваем количество примеров для быстрой оценки\n    eval_subset = eval_dataset.select(range(min(max_samples, len(eval_dataset))))\n    \n    with torch.no_grad():\n        for i, batch in enumerate(eval_subset):\n            if i % 10 == 0:\n                logger.info(f\"Обработано {i}/{len(eval_subset)} примеров\")\n            \n            # Подготавливаем входные данные\n            input_features = torch.tensor(batch[\"input_features\"]).unsqueeze(0).to(device)\n            \n            # Генерируем предсказание\n            predicted_ids = model.generate(\n                input_features,\n                max_length=225,\n                num_beams=1,\n                do_sample=False\n            )\n            \n            # Декодируем\n            pred_text = processor.tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n            true_text = processor.tokenizer.decode(batch[\"labels\"], skip_special_tokens=True)\n            \n            # Нормализация\n            if normalizer:\n                pred_text = normalizer(pred_text)\n                true_text = normalizer(true_text)\n            \n            predictions.append(pred_text)\n            references.append(true_text)\n    \n    # Вычисляем все метрики\n    results = {}\n    \n    # WER\n    wer = 100 * metrics_dict[\"wer\"].compute(predictions=predictions, references=references)\n    results[\"wer\"] = wer\n    \n    # CER\n    cer = 100 * metrics_dict[\"cer\"].compute(predictions=predictions, references=references)\n    results[\"cer\"] = cer\n    \n    # BLEU\n    try:\n        bleu = metrics_dict[\"bleu\"].compute(predictions=predictions, references=[[ref] for ref in references])\n        results[\"bleu\"] = bleu[\"bleu\"] * 100\n    except:\n        results[\"bleu\"] = 0.0\n    \n    # Детальная статистика ошибок\n    try:\n        all_preds = \" \".join(predictions)\n        all_refs = \" \".join(references)\n        measures = jiwer.compute_measures(all_refs, all_preds)\n        \n        results[\"substitutions\"] = measures[\"substitutions\"]\n        results[\"deletions\"] = measures[\"deletions\"]\n        results[\"insertions\"] = measures[\"insertions\"]\n        results[\"hits\"] = measures[\"hits\"]\n        \n        # Rates\n        total_words = measures[\"substitutions\"] + measures[\"deletions\"] + measures[\"hits\"]\n        if total_words > 0:\n            results[\"substitution_rate\"] = (measures[\"substitutions\"] / total_words) * 100\n            results[\"deletion_rate\"] = (measures[\"deletions\"] / total_words) * 100\n            results[\"insertion_rate\"] = (measures[\"insertions\"] / (total_words + measures[\"insertions\"])) * 100\n        \n    except Exception as e:\n        logger.warning(f\"Ошибка при вычислении детальных метрик: {e}\")\n    \n    # Sentence Accuracy\n    exact_matches = sum(1 for pred, ref in zip(predictions, references) if pred.strip() == ref.strip())\n    results[\"sentence_accuracy\"] = (exact_matches / len(predictions)) * 100\n    \n    # Длина предсказаний\n    avg_pred_length = sum(len(pred.split()) for pred in predictions) / len(predictions)\n    avg_ref_length = sum(len(ref.split()) for ref in references) / len(references)\n    results[\"avg_prediction_length\"] = avg_pred_length\n    results[\"avg_reference_length\"] = avg_ref_length\n    results[\"length_ratio\"] = avg_pred_length / avg_ref_length if avg_ref_length > 0 else 0.0\n    \n    # Выводим примеры и метрики\n    logger.info(\"\\n=== Примеры предсказаний ===\")\n    for i in range(min(5, len(predictions))):\n        logger.info(f\"Истинный текст: {references[i]}\")\n        logger.info(f\"Предсказание:   {predictions[i]}\")\n        logger.info(\"---\")\n    \n    logger.info(\"\\n=== Детальные метрики ===\")\n    logger.info(f\"WER: {results['wer']:.2f}%\")\n    logger.info(f\"CER: {results['cer']:.2f}%\")\n    logger.info(f\"BLEU: {results['bleu']:.2f}\")\n    logger.info(f\"Sentence Accuracy: {results['sentence_accuracy']:.2f}%\")\n    \n    if \"substitutions\" in results:\n        logger.info(f\"Substitutions: {results['substitutions']}\")\n        logger.info(f\"Deletions: {results['deletions']}\")\n        logger.info(f\"Insertions: {results['insertions']}\")\n        logger.info(f\"Hits: {results['hits']}\")\n        logger.info(f\"Substitution Rate: {results.get('substitution_rate', 0):.2f}%\")\n        logger.info(f\"Deletion Rate: {results.get('deletion_rate', 0):.2f}%\")\n        logger.info(f\"Insertion Rate: {results.get('insertion_rate', 0):.2f}%\")\n    \n    logger.info(f\"Avg Prediction Length: {results['avg_prediction_length']:.1f} words\")\n    logger.info(f\"Avg Reference Length: {results['avg_reference_length']:.1f} words\")\n    logger.info(f\"Length Ratio: {results['length_ratio']:.2f}\")\n    \n    results[\"predictions\"] = predictions\n    results[\"references\"] = references\n    \n    return results\n\nclass MetricsCallback(TrainerCallback):\n    \"\"\"Коллбэк для логирования расширенных метрик\"\"\"\n    \n    def on_evaluate(self, args, state, control, model, logs=None, **kwargs):\n        if logs:\n            logger.info(f\"\\nЭпоха {state.epoch} - Метрики оценки:\")\n            logger.info(f\"  WER: {logs.get('eval_wer', 'N/A'):.2f}%\")\n            logger.info(f\"  CER: {logs.get('eval_cer', 'N/A'):.2f}%\")\n            logger.info(f\"  BLEU: {logs.get('eval_bleu', 'N/A'):.2f}\")\n            logger.info(f\"  Sentence Accuracy: {logs.get('eval_sentence_accuracy', 'N/A'):.2f}%\")\n            \n            if 'eval_substitutions' in logs:\n                logger.info(f\"  Substitutions: {logs.get('eval_substitutions', 0)}\")\n                logger.info(f\"  Deletions: {logs.get('eval_deletions', 0)}\")\n                logger.info(f\"  Insertions: {logs.get('eval_insertions', 0)}\")\n                logger.info(f\"  Substitution Rate: {logs.get('eval_substitution_rate', 0):.2f}%\")\n\ndef main():\n    \"\"\"Основная функция файнтюнинга\"\"\"\n    \n    # Аргументы модели и данных\n    model_args = ModelArguments()\n    data_args = DataArguments()\n    \n    # Загружаем процессор и модель\n    logger.info(\"Загрузка модели и процессора...\")\n    \n    # Важно: правильно настраиваем язык и задачу\n    feature_extractor = WhisperFeatureExtractor.from_pretrained(model_args.model_name_or_path)\n    tokenizer = WhisperTokenizer.from_pretrained(\n        model_args.model_name_or_path, \n        language=\"ru\",  # Используем короткий код языка\n        task=\"transcribe\"\n    )\n    processor = WhisperProcessor.from_pretrained(\n        model_args.model_name_or_path, \n        language=\"ru\",  # Используем короткий код языка\n        task=\"transcribe\"\n    )\n    \n    model = WhisperForConditionalGeneration.from_pretrained(model_args.model_name_or_path)\n    model.to(device)\n    \n    # КРИТИЧЕСКИ ВАЖНО: правильная настройка модели для русского языка\n    model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\n        language=\"ru\", \n        task=\"transcribe\"\n    )\n    \n    # НЕ подавляем все токены - это может нарушить работу модели\n    # model.config.suppress_tokens = []\n    \n    logger.info(f\"Модель настроена для русского языка\")\n    logger.info(f\"Forced decoder IDs: {model.config.forced_decoder_ids}\")\n    \n    # Загружаем датасет\n    logger.info(\"Загрузка датасета...\")\n    raw_datasets = load_golos_dataset()\n    \n    # Определяем название текстовой колонки\n    sample = raw_datasets[\"train\"][0]\n    text_column_name = data_args.text_column_name\n    \n    # Проверяем, есть ли нужная колонка\n    if text_column_name not in sample:\n        # Ищем альтернативные названия\n        alternatives = [\"text\", \"transcription\", \"transcript\", \"target_text\", \"sentence\"]\n        for alt in alternatives:\n            if alt in sample:\n                text_column_name = alt\n                logger.info(f\"Используем колонку '{text_column_name}' для текста\")\n                break\n        else:\n            logger.error(f\"Не найдена текстовая колонка. Доступные: {list(sample.keys())}\")\n            return\n    \n    # Ресэмплируем аудио если необходимо\n    raw_datasets = raw_datasets.cast_column(\n        data_args.audio_column_name, \n        Audio(sampling_rate=feature_extractor.sampling_rate)\n    )\n    \n    # Нормализатор текста\n    normalizer = BasicTextNormalizer()\n    \n    # Предобработка данных\n    logger.info(\"Предобработка данных...\")\n    vectorized_datasets = raw_datasets.map(\n        lambda batch: prepare_dataset(batch, processor, normalizer, text_column_name),\n        remove_columns=raw_datasets[\"train\"].column_names,\n        desc=\"Предобработка данных\"\n    )\n    \n    # Ограничиваем размер датасета если указано\n    if data_args.max_train_samples:\n        vectorized_datasets[\"train\"] = vectorized_datasets[\"train\"].select(\n            range(min(data_args.max_train_samples, len(vectorized_datasets[\"train\"])))\n        )\n    \n    if data_args.max_eval_samples:\n        vectorized_datasets[\"validation\"] = vectorized_datasets[\"validation\"].select(\n            range(min(data_args.max_eval_samples, len(vectorized_datasets[\"validation\"])))\n        )\n    \n    # Коллатор данных\n    data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n        processor=processor,\n        decoder_start_token_id=model.config.decoder_start_token_id,\n    )\n    \n    # Загружаем все метрики\n    logger.info(\"Инициализация метрик...\")\n    metrics_dict = {\n        \"wer\": evaluate.load(\"wer\"),\n        \"cer\": evaluate.load(\"cer\"),\n        \"bleu\": evaluate.load(\"bleu\")\n    }\n    \n    logger.info(\"Доступные метрики: WER, CER, BLEU, Sentence Accuracy, подробная статистика ошибок\")\n    \n    # Оценка до файнтюнинга\n    logger.info(\"\\n\" + \"=\"*50)\n    logger.info(\"ОЦЕНКА МОДЕЛИ ДО ФАЙНТЮНИНГА\")\n    logger.info(\"=\"*50)\n    \n    pre_finetune_results = evaluate_model(\n        model, processor, vectorized_datasets[\"validation\"], \n        normalizer, metrics_dict, max_samples=20\n    )\n    logger.info(f\"WER до файнтюнинга: {pre_finetune_results['wer']:.2f}%\")\n    \n    # Функция для вычисления метрик\n    def compute_metrics_wrapper(eval_preds):\n        return compute_metrics(eval_preds, processor, normalizer, metrics_dict)\n    \n    # Аргументы обучения\n    training_args = Seq2SeqTrainingArguments(\n        output_dir=\"./whisper-golos-finetuned\",\n        per_device_train_batch_size=4,  # Уменьшаем batch size\n        per_device_eval_batch_size=4,\n        gradient_accumulation_steps=4,  # Увеличиваем накопление градиентов\n        learning_rate=5e-6,  # Более консервативный learning rate\n        warmup_steps=100,\n        max_steps=1000,  # Меньше шагов для демонстрации\n        gradient_checkpointing=True,\n        fp16=True if device == \"cuda\" else False,\n        eval_steps=200,\n        save_strategy=\"steps\",\n        save_steps=200,\n        logging_steps=50,\n        report_to=[\"tensorboard\"],\n        metric_for_best_model=\"wer\",\n        greater_is_better=False,\n        push_to_hub=False,\n        dataloader_num_workers=0,\n        predict_with_generate=True,\n        generation_max_length=225,\n        generation_num_beams=2,  # Beam search для лучшего качества\n        save_total_limit=2,\n        # Добавляем параметры для стабильности обучения\n        dataloader_drop_last=True,\n        remove_unused_columns=False,\n    )\n    \n    # Создаем тренер\n    trainer = Seq2SeqTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=vectorized_datasets[\"train\"],\n        eval_dataset=vectorized_datasets[\"validation\"],\n        tokenizer=processor.feature_extractor,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics_wrapper,\n        callbacks=[MetricsCallback()],\n    )\n    \n    # Обучение\n    logger.info(\"\\n\" + \"=\"*50)\n    logger.info(\"НАЧАЛО ФАЙНТЮНИНГА\")\n    logger.info(\"=\"*50)\n    \n    try:\n        train_result = trainer.train()\n        logger.info(f\"Обучение завершено за {train_result.metrics['train_runtime']:.2f} секунд\")\n    except Exception as e:\n        logger.error(f\"Ошибка во время обучения: {e}\")\n        return\n    \n    # Оценка после файнтюнинга\n    logger.info(\"\\n\" + \"=\"*50)\n    logger.info(\"ОЦЕНКА МОДЕЛИ ПОСЛЕ ФАЙНТЮНИНГА\")\n    logger.info(\"=\"*50)\n    \n    post_finetune_results = evaluate_model(\n        model, processor, vectorized_datasets[\"validation\"], \n        normalizer, metrics_dict, max_samples=20\n    )\n    logger.info(f\"WER после файнтюнинга: {post_finetune_results['wer']:.2f}%\")\n    \n    # Сравнение результатов\n    wer_improvement = pre_finetune_results['wer'] - post_finetune_results['wer']\n    cer_improvement = pre_finetune_results['cer'] - post_finetune_results['cer']\n    bleu_improvement = post_finetune_results['bleu'] - pre_finetune_results['bleu']\n    accuracy_improvement = post_finetune_results['sentence_accuracy'] - pre_finetune_results['sentence_accuracy']\n    \n    logger.info(\"\\n\" + \"=\"*60)\n    logger.info(\"ИТОГОВЫЕ РЕЗУЛЬТАТЫ\")\n    logger.info(\"=\"*60)\n    logger.info(\"ОСНОВНЫЕ МЕТРИКИ:\")\n    logger.info(f\"  WER до файнтюнинга:    {pre_finetune_results['wer']:.2f}%\")\n    logger.info(f\"  WER после файнтюнинга: {post_finetune_results['wer']:.2f}%\")\n    logger.info(f\"  Улучшение WER:         {wer_improvement:.2f}% {'(лучше)' if wer_improvement > 0 else '(хуже)'}\")\n    logger.info(\"\")\n    logger.info(f\"  CER до файнтюнинга:    {pre_finetune_results['cer']:.2f}%\")\n    logger.info(f\"  CER после файнтюнинга: {post_finetune_results['cer']:.2f}%\")\n    logger.info(f\"  Улучшение CER:         {cer_improvement:.2f}% {'(лучше)' if cer_improvement > 0 else '(хуже)'}\")\n    logger.info(\"\")\n    logger.info(f\"  BLEU до файнтюнинга:    {pre_finetune_results['bleu']:.2f}\")\n    logger.info(f\"  BLEU после файнтюнинга: {post_finetune_results['bleu']:.2f}\")\n    logger.info(f\"  Улучшение BLEU:         {bleu_improvement:.2f} {'(лучше)' if bleu_improvement > 0 else '(хуже)'}\")\n    logger.info(\"\")\n    logger.info(f\"  Sentence Accuracy до:    {pre_finetune_results['sentence_accuracy']:.2f}%\")\n    logger.info(f\"  Sentence Accuracy после: {post_finetune_results['sentence_accuracy']:.2f}%\")\n    logger.info(f\"  Улучшение Accuracy:      {accuracy_improvement:.2f}% {'(лучше)' if accuracy_improvement > 0 else '(хуже)'}\")\n    \n    # Детальная статистика ошибок\n    if 'substitutions' in post_finetune_results:\n        logger.info(\"\\nДЕТАЛЬНАЯ СТАТИСТИКА ОШИБОК (после файнтюнинга):\")\n        logger.info(f\"  Правильно распознано слов: {post_finetune_results['hits']}\")\n        logger.info(f\"  Замены (substitutions):    {post_finetune_results['substitutions']}\")\n        logger.info(f\"  Удаления (deletions):      {post_finetune_results['deletions']}\")\n        logger.info(f\"  Вставки (insertions):      {post_finetune_results['insertions']}\")\n        logger.info(f\"  Частота замен:             {post_finetune_results.get('substitution_rate', 0):.2f}%\")\n        logger.info(f\"  Частота удалений:          {post_finetune_results.get('deletion_rate', 0):.2f}%\")\n        logger.info(f\"  Частота вставок:           {post_finetune_results.get('insertion_rate', 0):.2f}%\")\n    \n    logger.info(\"\\nАНАЛИЗ ДЛИНЫ ПРЕДСКАЗАНИЙ:\")\n    logger.info(f\"  Средняя длина референса:    {post_finetune_results['avg_reference_length']:.1f} слов\")\n    logger.info(f\"  Средняя длина предсказания: {post_finetune_results['avg_prediction_length']:.1f} слов\")\n    logger.info(f\"  Соотношение длин:           {post_finetune_results['length_ratio']:.2f}\")\n    logger.info(\"=\"*60)\n    \n    # Сохранение модели\n    logger.info(\"\\nСохранение модели...\")\n    model_save_path = \"./whisper_golos.pt\"\n    \n    # Сохраняем состояние модели\n    torch.save({\n        'model_state_dict': model.state_dict(),\n        'config': model.config,\n        'metrics_before': {\n            'wer': pre_finetune_results['wer'],\n            'cer': pre_finetune_results['cer'],\n            'bleu': pre_finetune_results['bleu'],\n            'sentence_accuracy': pre_finetune_results['sentence_accuracy']\n        },\n        'metrics_after': {\n            'wer': post_finetune_results['wer'],\n            'cer': post_finetune_results['cer'],\n            'bleu': post_finetune_results['bleu'],\n            'sentence_accuracy': post_finetune_results['sentence_accuracy']\n        },\n        'improvements': {\n            'wer': wer_improvement,\n            'cer': cer_improvement,\n            'bleu': bleu_improvement,\n            'sentence_accuracy': accuracy_improvement\n        }\n    }, model_save_path)\n    \n    # Также сохраняем полную модель в формате HuggingFace\n    model.save_pretrained(\"./whisper-golos-final\")\n    processor.save_pretrained(\"./whisper-golos-final\")\n    \n    logger.info(f\"Модель сохранена в {model_save_path}\")\n    logger.info(\"Полная модель сохранена в ./whisper-golos-final/\")\n    \n    # Сохраняем детальные результаты\n    results = {\n        \"model_name\": model_args.model_name_or_path,\n        \"dataset\": data_args.dataset_name,\n        \"metrics_before_finetune\": {\n            \"wer\": pre_finetune_results['wer'],\n            \"cer\": pre_finetune_results['cer'],\n            \"bleu\": pre_finetune_results['bleu'],\n            \"sentence_accuracy\": pre_finetune_results['sentence_accuracy'],\n            \"avg_prediction_length\": pre_finetune_results['avg_prediction_length'],\n            \"avg_reference_length\": pre_finetune_results['avg_reference_length'],\n            \"length_ratio\": pre_finetune_results['length_ratio']\n        },\n        \"metrics_after_finetune\": {\n            \"wer\": post_finetune_results['wer'],\n            \"cer\": post_finetune_results['cer'],\n            \"bleu\": post_finetune_results['bleu'],\n            \"sentence_accuracy\": post_finetune_results['sentence_accuracy'],\n            \"avg_prediction_length\": post_finetune_results['avg_prediction_length'],\n            \"avg_reference_length\": post_finetune_results['avg_reference_length'],\n            \"length_ratio\": post_finetune_results['length_ratio']\n        },\n        \"improvements\": {\n            \"wer\": wer_improvement,\n            \"cer\": cer_improvement,\n            \"bleu\": bleu_improvement,\n            \"sentence_accuracy\": accuracy_improvement\n        },\n        \"training_steps\": training_args.max_steps,\n        \"examples\": {\n            \"references\": post_finetune_results['references'][:5],\n            \"predictions\": post_finetune_results['predictions'][:5]\n        }\n    }\n    \n    # Добавляем детальную статистику ошибок если доступна\n    if 'substitutions' in post_finetune_results:\n        results[\"error_analysis\"] = {\n            \"hits\": post_finetune_results['hits'],\n            \"substitutions\": post_finetune_results['substitutions'],\n            \"deletions\": post_finetune_results['deletions'],\n            \"insertions\": post_finetune_results['insertions'],\n            \"substitution_rate\": post_finetune_results.get('substitution_rate', 0),\n            \"deletion_rate\": post_finetune_results.get('deletion_rate', 0),\n            \"insertion_rate\": post_finetune_results.get('insertion_rate', 0)\n        }\n        \n        results[\"examples\"] = {\n            \"references\": post_finetune_results['references'][:5],\n            \"predictions\": post_finetune_results['predictions'][:5]\n        }\n    \n    with open(\"finetune_results.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(results, f, ensure_ascii=False, indent=2)\n    \n    logger.info(\"Детальные результаты сохранены в finetune_results.json\")\n    logger.info(\"Файнтюнинг завершен успешно!\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T11:25:00.865514Z","iopub.execute_input":"2025-05-23T11:25:00.865812Z","iopub.status.idle":"2025-05-23T13:22:50.822575Z","shell.execute_reply.started":"2025-05-23T11:25:00.865789Z","shell.execute_reply":"2025-05-23T13:22:50.821967Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/3084336095.py:645: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 1:57:06, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.011000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.409500</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.281900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.248400</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.273000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.242300</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.255500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.235000</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.234400</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.220900</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.205000</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.213500</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.108700</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.129500</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.117500</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.113000</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.113300</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.111200</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.108600</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.108900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(path, download_file_name):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip {zip_name} {path} -r\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T13:46:26.885923Z","iopub.execute_input":"2025-05-23T13:46:26.886243Z","iopub.status.idle":"2025-05-23T13:46:26.890955Z","shell.execute_reply.started":"2025-05-23T13:46:26.886221Z","shell.execute_reply":"2025-05-23T13:46:26.890347Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"download_file('/kaggle/working/whisper-golos-final', 'whisper_final')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T13:56:22.987065Z","iopub.execute_input":"2025-05-23T13:56:22.987591Z","iopub.status.idle":"2025-05-23T13:57:13.233217Z","shell.execute_reply.started":"2025-05-23T13:56:22.987568Z","shell.execute_reply":"2025-05-23T13:57:13.232430Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"/kaggle/working/whisper_final.zip","text/html":"<a href='whisper_final.zip' target='_blank'>whisper_final.zip</a><br>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}